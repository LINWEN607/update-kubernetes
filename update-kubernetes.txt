操作系统：CentOS Linux 7 (Core)
Containerd：1.6.33
Kubernetes：1.30.1 - 1.31.0
1、升级 control plane nodes
1.1、kubeadm 升级





好习惯先备份etcd
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
snapshot save /path/to/backup/snapshot.db
踩坑可恢复
ETCDCTL_API=3 etcdctl snapshot restore /backup/etcd-snapshot-20250626.db \
--name etcd1 \
--initial-cluster etcd1=http://192.168.1.1:2380,etcd2=http://192.168.1.2:2380,etcd3=http://192.168.1.3:2380 \
--initial-cluster-token my-etcd-cluster-token \
--initial-advertise-peer-urls http://192.168.1.1:2380 \
--data-dir /var/lib/etcd


/path/to/backup/snapshot.db:

    这是你之前使用 etcdctl snapshot save 创建的快照文件的完整路径。
    示例：/backup/etcd-snapshot-20250626.db

--name <etcd-member-name>:

    指定要恢复的 etcd 成员的名字。
    在多成员的 etcd 集群中，每个成员都有一个唯一的名称。
    示例：如果你的 etcd 成员名是 etcd1，则该参数应为 --name etcd1

--initial-cluster <member1=http://<ip1>:2380,member2=http://<ip2>:2380,...>:

    定义了整个 etcd 集群的所有成员及其通信地址（peer URLs）。
    对于单节点集群，这只需包含当前节点的信息；对于多节点集群，则需列出所有节点的信息。
    示例：在三节点集群中，如果节点分别是 etcd1, etcd2, 和 etcd3，其 IP 分别为 192.168.1.1, 192.168.1.2, 和 192.168.1.3，则该参数应为：

    深色版本

    --initial-cluster etcd1=http://192.168.1.1:2380,etcd2=http://192.168.1.2:2380,etcd3=http://192.168.1.3:2380

--initial-cluster-token <token>:

    用于标识集群的唯一令牌。这对于首次启动一个新的集群非常重要，但在从备份恢复时，这个值可以随意设置，因为它主要影响的是新集群的初始化而不是已有数据的恢复。
    示例：my-etcd-cluster-token

--initial-advertise-peer-urls http://<local-ip>:2380:

    当前 etcd 成员对外广播的 peer URL 地址，其他成员将通过此地址与之通信。
    <local-ip> 应替换为运行此命令的机器的实际 IP 地址。
    示例：如果当前机器的 IP 是 192.168.1.1，则该参数应为 --initial-advertise-peer-urls http://192.168.1.1:2380

--data-dir /var/lib/etcd:

    指定 etcd 数据存储目录的位置。恢复后的数据将被放置在此目录下。
    根据你的实际情况调整路径，默认通常是 /var/lib/etcd。














a、查看当前节点状态
# 查看当前节点状态及版本，kubeadm\kubelet\kubectl 版本
# 此处举例升级一个控制节点
kubectl get node
kubeadm version
kubelet version
kubectl version








b、更改 Kubernetes 软件包仓库
# 查看当前配置为 1.30 的 Kubernetes 软件包仓库
cat /etc/yum.repos.d/kubernetes.repo
yum list kubeadm --showduplicates
 
# 备份原有配置
mv /etc/yum.repos.d/kubernetes.repo /etc/yum.repos.d/kubernetes.repo.bak
 
# 更改 Kubernetes 软件包仓库，此处配置阿里的源
cat <<EOF | tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.31/rpm/
enabled=1
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.31/rpm/repodata/repomd.xml.key
EOF
 
cat /etc/yum.repos.d/kubernetes.repo









c、升级 kubeadm
# 添加新 .repo 文件后，清理 YUM 的缓存，以便可识别新的软件源
yum clean all
 
# 升级 kubeadm
yum install -y kubeadm-1.31.0
 
# 验证下载操作正常，并且 kubeadm 版本正确
kubeadm version
 
# 验证升级计划
kubeadm upgrade plan
 
# 运行命令，升级到的目标版本
kubeadm upgrade apply v1.31.0
# （可选）若不升级 etcd，选择下方命令
kubeadm upgrade apply v1.31.0 --etcd-upgrade=false
 
# 注意：
# 其他控制节点，使用 kubeadm upgrade node，不使用 kubeadm upgrade apply 命令
# 并且不需要执行 kubeadm upgrade plan 和更新 CNI 驱动插件的操作








1.2 kubelet、kubectl 升级
a、升级 kubelet、kubectl
# 将节点标记为不可调度并驱逐所有负载
kubectl get node
kubectl drain master --ignore-daemonsets
 
# 升级 kubelet 和 kubectl
yum install -y kubelet-1.31.0 kubectl-1.31.0
 
# 重启 kubelet
systemctl daemon-reload 
systemctl restart kubelet
 
# 验证 kubelet、kubectl 升级成功
kubelet --version 
kubectl version
 
# 解除 master 节点的保护，恢复调度
kubectl uncordon master
 
# 查看 master 状态为 ready，所有 Pod 状态 runnig
kubectl get nodes
kubectl get pod -A






2、升级 Linux nodes
### 升级工作节点 ###
# 更改 Kubernetes 软件包仓库，可见 1->1.1->b 部分内容
 
# 升级 kubeadm
yum install -y kubeadm-1.31.0
 
# 执行 kubeadm upgrade，升级本地的 kubelet 配置
kubeadm upgrade node
 
# 将节点标记为不可调度并驱逐所有负载，准备节点的维护
# 在控制平面节点上执行此命令，<node-to-drain> 为正腾空的节点名称
kubectl drain <node-to-drain> --ignore-daemonsets
 
# 升级 kubelet 和 kubectl
yum install -y kubelet-1.31.0 kubectl-1.31.0
 
# 重启 kubelet
systemctl daemon-reload
systemctl restart kubelet
 
# 取消对节点的保护
# 在控制平面节点上执行此命令，<node-to-uncordon> 为节点名称
kubectl uncordon <node-to-uncordon>
